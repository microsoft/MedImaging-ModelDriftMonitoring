{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Drift Experiment Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Needs to be double checked*\n",
    "\n",
    "**Goal**: To identify drift from streams of unseen chest x-ray images\n",
    "\n",
    "**Mehods:**\n",
    "\n",
    "1. **Statistical drift detection:** Distance-based measure like Maximum Mean Discrepancy (MMD) to determine the separation between training (reference) data and unseen chest x-rays: \n",
    "   - For image data, we will reduce the dimensionality before running the statistical test. \n",
    "   - We then run standard CheXpert pre-processing steps and train a drift detector. \n",
    "   - We detect data drift by predicting on a batch of x-ray images (spread out over a pre-defined period of time). \n",
    "   - We return the **p-value and the threshold** of the test that results in a drift declaration.\n",
    "\n",
    "\n",
    "2. **Artificial Neural Network (ANN) based drift detection:** Train an autoencoder to learn how to efficiently compress and encode reference data:\n",
    "   - The AE detector tries to reconstruct the input it receives.\n",
    "   -  If the unseen, input x-ray cannot be reconstructed well, the reconstruction error is high and the data can be flagged as an outlier (drift).\n",
    "   - The reconstruction error is measured as the mean squared error (MSE) between the input and the reconstructed instance.\n",
    "\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "- CheXpert training data (reference data)\n",
    "- Padchest filtered/curated data (new data to be probed for drift)\n",
    "- Alibi Detect Python library (package with boilerplate code to facilitate methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (cloudpickle 2.0.0 (d:\\code\\mlopsday2\\medimaging-modeldriftmonitoring\\.venv\\lib\\site-packages), Requirement.parse('cloudpickle<2.0.0,>=1.1.0'), {'azureml-dataprep'}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML SDK Version:  1.34.0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import azureml\n",
    "from IPython.display import display, Markdown\n",
    "from azureml.core import Datastore, Experiment, ScriptRunConfig, Workspace, RunConfiguration\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.runconfig import DockerConfiguration\n",
    "from azureml.exceptions import UserErrorException\n",
    "\n",
    "from model_drift import settings\n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library configuration succeeded\n",
      "Workspace: MLOps_shared\n"
     ]
    }
   ],
   "source": [
    "# Connect to workspace\n",
    "subscription_id = '9ca8df1a-bf40-49c6-a13f-66b72a85f43c'\n",
    "resource_group = 'MLOps-Prototype'\n",
    "workspace_name = 'MLOps_shared'\n",
    "\n",
    "try:\n",
    "    ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
    "    ws.write_config()\n",
    "    print('Library configuration succeeded')\n",
    "except:\n",
    "    print('Workspace not found')\n",
    "\n",
    "print(\"Workspace:\", ws.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing dataset\n"
     ]
    }
   ],
   "source": [
    "# Get Dataset\n",
    "try:\n",
    "    chex_dataset = Dataset.get_by_name(ws, name='chexpert')\n",
    "    print('Found existing dataset')\n",
    "except UserErrorException:\n",
    "    # Import a FileDataset pointing to files in 'png' folder and its subfolders recursively\n",
    "    datastore = Datastore.get(ws, 'chexpert')\n",
    "    datastore_paths = [(datastore, '/')]\n",
    "    chex_dataset = Dataset.File.from_files(path=datastore_paths)\n",
    "    print('Created Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"vae\"\n",
    "\n",
    "environment_file = settings.CONDA_ENVIRONMENT_FILE\n",
    "project_dir = Path(\"./experiment\")\n",
    "pytorch_env = Environment.from_conda_specification(env_name, file_path =str(environment_file))\n",
    "\n",
    "pytorch_env.register(workspace=ws)\n",
    "build = pytorch_env.build(workspace=ws)\n",
    "\n",
    "pytorch_env.environment_variables[\"RSLEX_DIRECT_VOLUME_MOUNT\"] = \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data files into experiement directory\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "valid_fn = settings.CHEXPERT_VALID_CSV\n",
    "train_fn = settings.CHEXPERT_TRAIN_CSV\n",
    "\n",
    "shutil.copy(valid_fn, project_dir.joinpath(\"valid.csv\"))\n",
    "shutil.copy(train_fn, project_dir.joinpath(\"train.csv\"))\n",
    "\n",
    "df = pd.read_csv(train_fn)\n",
    "df = df[df['Frontal/Lateral'] != 'Lateral']\n",
    "df.to_csv(project_dir.joinpath(\"train-frontal-only.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: chexpert-vae\n",
      "Compute Target: nc24-uswest2\n",
      "Environment: vae\n"
     ]
    }
   ],
   "source": [
    "dbg = False\n",
    "\n",
    "log_refresh_rate = 25\n",
    "if dbg:\n",
    "    log_refresh_rate = 2\n",
    "\n",
    "# Name experiement\n",
    "experiment_name = 'chexpert-vae' if not dbg else 'chexpert-vae-dbg'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)\n",
    "\n",
    "print(\"Experiment:\", exp.name)\n",
    "print(\"Environment:\", pytorch_env.name)\n",
    "\n",
    "run_config = RunConfiguration()\n",
    "\n",
    "run_config.environment = pytorch_env\n",
    "run_config.docker = DockerConfiguration(use_docker=True, shm_size=\"100G\")\n",
    "run_config.target = cluster_name\n",
    "\n",
    "\n",
    "args = [\n",
    "    '--data_folder', chex_dataset.as_named_input('chexpertv1').as_mount(),\n",
    "    '--run_azure', 1,\n",
    "    '--output_dir', './outputs',\n",
    "\n",
    "    '--batch_size', 32,\n",
    "    \"--base_lr\", 1e-4,\n",
    "\n",
    "    \"--image_size\", 128,\n",
    "    '--max_epochs', 50 if not dbg else 5,\n",
    "    '--num_workers', -1,\n",
    "\n",
    "    '--progress_bar_refresh_rate', log_refresh_rate,\n",
    "    \"--log_every_n_steps\", log_refresh_rate,\n",
    "    \"--flush_logs_every_n_steps\", log_refresh_rate,\n",
    "    \"--accelerator\", \"ddp\",\n",
    "    \"--channels\", 1,\n",
    "\n",
    "    \"--step_size\", 3,\n",
    "    \"--lr_scheduler\", \"plateau\",\n",
    "\n",
    "    \"--auto_scale_batch_size\", False,\n",
    "    \"--auto_lr_find\", False,\n",
    "    \"--train_csv\", \"train-frontal-only.csv\",\n",
    "\n",
    "    \"--width\", 320,\n",
    "    \"--z\", 64,\n",
    "    \"--layer_count\", 3,\n",
    "\n",
    "    \"--terminate_on_nan\", True,\n",
    "\n",
    "    \"--log_recon_images\", 32\n",
    "    ]\n",
    "\n",
    "if dbg:\n",
    "\n",
    "    args += [\n",
    "        '--limit_train_batches', 5,\n",
    "    ]\n",
    "\n",
    "\n",
    "config = ScriptRunConfig(\n",
    "    source_directory = str(project_dir), \n",
    "    script = \"train.py\",\n",
    "    arguments=args,\n",
    ")\n",
    "\n",
    "config.run_config = run_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Submitting d:\\Code\\MLOpsDay2\\ModelMonitoring\\Model_Monitoring\\1.Experiment1_input_analysis\\image_based_drift\\experiment directory for run. The size of the directory >= 25 MB, so it can take a few minutes.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "- Experiement: [chexpert-vae](https://ml.azure.com/experiments/chexpert-vae?wsid=/subscriptions/9ca8df1a-bf40-49c6-a13f-66b72a85f43c/resourcegroups/MLOps-Prototype/workspaces/MLOps_shared&tid=72f988bf-86f1-41af-91ab-2d7cd011db47)\n",
       "- Run: [dreamy_yogurt_j14cm892](https://ml.azure.com/runs/chexpert-vae_1632705750_17ad5846?wsid=/subscriptions/9ca8df1a-bf40-49c6-a13f-66b72a85f43c/resourcegroups/MLOps-Prototype/workspaces/MLOps_shared&tid=72f988bf-86f1-41af-91ab-2d7cd011db47)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config.run_config.target = \"nc24-uswest2\"\n",
    "\n",
    "run = exp.submit(config)\n",
    "display(Markdown(f\"\"\"\n",
    "- Experiement: [{run.experiment.name}]({run.experiment.get_portal_url()})\n",
    "- Run: [{run.display_name}]({run.get_portal_url()})\n",
    "- Target: {config.run_config.target}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain hyper drive\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import GridParameterSampling, RandomParameterSampling, BanditPolicy, HyperDriveConfig, uniform, PrimaryMetricGoal, choice, loguniform\n",
    "run_config = RunConfiguration()\n",
    "\n",
    "cluster_name = \"NC24s-v2-usw2-lp\"\n",
    "\n",
    "run_config.environment = pytorch_env\n",
    "run_config.docker = DockerConfiguration(use_docker=True, shm_size=\"100G\")\n",
    "run_config.target = cluster_name\n",
    "\n",
    "\n",
    "param_sampling = RandomParameterSampling(\n",
    "    {\n",
    "        \"layer\": choice(3, 4),\n",
    "        \"batch_size\": choice(16, 32),\n",
    "        \"image_size\": choice(128,256),\n",
    "        \"z\": choice(32, 64, 128),\n",
    "        \"width\": choice(160, 240, 320),\n",
    "        \"base_lr\": choice(1e-4, 1e-5, 1e-6),\n",
    "        \"kl_coeff\": choice(0.1)\n",
    "    }\n",
    ")\n",
    "\n",
    "experiment_name = 'chexpert-vae-tune'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)\n",
    "config.run_config = run_config\n",
    "hyperdrive_config = HyperDriveConfig(run_config=config,\n",
    "                                     hyperparameter_sampling=param_sampling, \n",
    "                                     policy=None,\n",
    "                                     primary_metric_name='val/recon_loss_frontal',\n",
    "                                     primary_metric_goal=PrimaryMetricGoal.MINIMIZE,\n",
    "                                     max_total_runs=6*12,\n",
    "                                     max_concurrent_runs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "- Experiement: [chexpert-vae-tune](https://ml.azure.com/experiments/chexpert-vae-tune?wsid=/subscriptions/9ca8df1a-bf40-49c6-a13f-66b72a85f43c/resourcegroups/MLOps-Prototype/workspaces/MLOps_shared&tid=72f988bf-86f1-41af-91ab-2d7cd011db47)\n",
       "- Run: [sleepy_leg_4q5lsqsg](https://ml.azure.com/runs/HD_bdd55aa3-b6c1-4a43-89b8-c5b00b22c4a3?wsid=/subscriptions/9ca8df1a-bf40-49c6-a13f-66b72a85f43c/resourcegroups/MLOps-Prototype/workspaces/MLOps_shared&tid=72f988bf-86f1-41af-91ab-2d7cd011db47)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start the HyperDrive run\n",
    "hyperdrive_run = exp.submit(hyperdrive_config)\n",
    "display(Markdown(f\"\"\"\n",
    "- Experiement: [{hyperdrive_run.experiment.name}]({hyperdrive_run.experiment.get_portal_url()})\n",
    "- Run: [{hyperdrive_run.display_name}]({hyperdrive_run.get_portal_url()})\n",
    "- Target: {config.run_config.target}\n",
    "\"\"\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40209cfd1e49aba1e20a3908f9a243f43b2ed73034fd3a81730d62124bbdcdae"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
