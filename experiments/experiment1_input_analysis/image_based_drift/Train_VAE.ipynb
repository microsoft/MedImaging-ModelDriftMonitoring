{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Drift Experiment Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Needs to be double checked*\n",
    "\n",
    "**Goal**: To identify drift from streams of unseen chest x-ray images\n",
    "\n",
    "**Mehods:**\n",
    "\n",
    "1. **Statistical drift detection:** Distance-based measure like Maximum Mean Discrepancy (MMD) to determine the separation between training (reference) data and unseen chest x-rays: \n",
    "   - For image data, we will reduce the dimensionality before running the statistical test. \n",
    "   - We then run standard CheXpert pre-processing steps and train a drift detector. \n",
    "   - We detect data drift by predicting on a batch of x-ray images (spread out over a pre-defined period of time). \n",
    "   - We return the **p-value and the threshold** of the test that results in a drift declaration.\n",
    "\n",
    "\n",
    "2. **Artificial Neural Network (ANN) based drift detection:** Train an autoencoder to learn how to efficiently compress and encode reference data:\n",
    "   - The AE detector tries to reconstruct the input it receives.\n",
    "   -  If the unseen, input x-ray cannot be reconstructed well, the reconstruction error is high and the data can be flagged as an outlier (drift).\n",
    "   - The reconstruction error is measured as the mean squared error (MSE) between the input and the reconstructed instance.\n",
    "\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "- CheXpert training data (reference data)\n",
    "- Padchest filtered/curated data (new data to be probed for drift)\n",
    "- Alibi Detect Python library (package with boilerplate code to facilitate methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (cloudpickle 2.0.0 (d:\\code\\mlopsday2\\medimaging-modeldriftmonitoring\\.venv\\lib\\site-packages), Requirement.parse('cloudpickle<2.0.0,>=1.1.0'), {'azureml-dataprep'}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML SDK Version:  1.35.0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import azureml\n",
    "from IPython.display import display, Markdown\n",
    "from azureml.core import Datastore, Experiment, ScriptRunConfig, Workspace, RunConfiguration\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.runconfig import DockerConfiguration\n",
    "from azureml.exceptions import UserErrorException\n",
    "import shutil\n",
    "\n",
    "\n",
    "from model_drift import settings, helpers\n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    }
   ],
   "source": [
    "# Connect to workspace\n",
    "ws = Workspace.from_config(settings.AZUREML_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args:\n",
      " accelerator: ddp\n",
      " auto_lr_find: False\n",
      " auto_scale_batch_size: False\n",
      " base_lr: 0.0001\n",
      " batch_size: 32\n",
      " channels: 1\n",
      " data_folder: <azureml.data.dataset_consumption_config.DatasetConsumptionConfig object at 0x000002A1052BED68>\n",
      " dataset: chexpert\n",
      " flush_logs_every_n_steps: 25\n",
      " frontal_only: 0\n",
      " ignore_nonfrontal_loss: 0\n",
      " image_size: 128\n",
      " layer_count: 3\n",
      " log_every_n_steps: 25\n",
      " log_recon_images: 32\n",
      " lr_scheduler: plateau\n",
      " max_epochs: 50\n",
      " normalize: False\n",
      " num_workers: -1\n",
      " output_dir: ./outputs\n",
      " progress_bar_refresh_rate: 25\n",
      " run_azure: 1\n",
      " step_size: 3\n",
      " terminate_on_nan: True\n",
      " val_frontal_only: 0\n",
      " width: 320\n",
      " z: 64\n",
      "Environment: vae\n",
      "Experiment: vae-chexpert\n"
     ]
    }
   ],
   "source": [
    "dbg = False\n",
    "\n",
    "log_refresh_rate = 25\n",
    "if dbg:\n",
    "    log_refresh_rate = 1\n",
    "\n",
    "# Name experiement\n",
    "\n",
    "dataset_name = \"chexpert\"\n",
    "# dataset_name = \"padchest\"\n",
    "env_name = \"vae\"\n",
    "experiment_name = f'vae-{dataset_name}' if not dbg else f'vae-{dataset_name}-dbg'\n",
    "\n",
    "\n",
    "# Input Dataset\n",
    "dataset = Dataset.get_by_name(ws, name=dataset_name)\n",
    "\n",
    "#Experiment\n",
    "exp = Experiment(workspace=ws, name=experiment_name)\n",
    "\n",
    "#Environment\n",
    "environment_file = settings.CONDA_ENVIRONMENT_FILE\n",
    "project_dir = settings.SRC_DIR\n",
    "pytorch_env = Environment.from_conda_specification(env_name, file_path =str(environment_file))\n",
    "pytorch_env.register(workspace=ws)\n",
    "build = pytorch_env.build(workspace=ws)\n",
    "pytorch_env.environment_variables[\"RSLEX_DIRECT_VOLUME_MOUNT\"] = \"True\"\n",
    "\n",
    "# Run Configuration\n",
    "run_config = RunConfiguration()\n",
    "run_config.environment_variables[\"RSLEX_DIRECT_VOLUME_MOUNT\"] = \"True\"\n",
    "run_config.environment = pytorch_env\n",
    "run_config.docker = DockerConfiguration(use_docker=True, shm_size=\"100G\")\n",
    "\n",
    "\n",
    "args = {\n",
    " 'dataset': dataset_name,\n",
    " 'run_azure': 1,\n",
    " 'output_dir': './outputs',\n",
    "\n",
    " 'frontal_only': 0,\n",
    " 'val_frontal_only': 0,\n",
    " 'ignore_nonfrontal_loss': 0,\n",
    " \n",
    " 'batch_size': 32,\n",
    " 'base_lr': 0.0001,\n",
    " 'image_size': 128,\n",
    " \n",
    " 'max_epochs': 50 if not dbg else 5,\n",
    " 'num_workers': -1,\n",
    " \n",
    " 'progress_bar_refresh_rate': log_refresh_rate,\n",
    " 'log_every_n_steps': log_refresh_rate,\n",
    " 'flush_logs_every_n_steps': log_refresh_rate,\n",
    "\n",
    " 'accelerator': 'ddp',\n",
    " 'channels': 1,\n",
    " 'normalize': False,\n",
    " \n",
    " 'step_size': 3,\n",
    " 'lr_scheduler': 'plateau',\n",
    " 'auto_scale_batch_size': False,\n",
    " 'auto_lr_find': False,\n",
    " \n",
    " 'width': 320,\n",
    " 'z': 64,\n",
    " 'layer_count': 3,\n",
    " 'terminate_on_nan': True,\n",
    " 'log_recon_images': 32\n",
    " }\n",
    "\n",
    "if dbg:\n",
    "    args.update({\n",
    "        'limit_train_batches': 5,\n",
    "        'limit_val_batches': 5,\n",
    "        'num_sanity_val_steps': 5\n",
    "    })\n",
    "\n",
    "\n",
    "args['data_folder'] = Dataset.get_by_name(ws, name=args['dataset']).as_named_input('dataset').as_mount()\n",
    "\n",
    "print(\"args:\")\n",
    "for k,v in sorted(args.items()):\n",
    "    print(f\" {k}: {v}\")\n",
    "\n",
    "\n",
    "print(f\"Environment: {pytorch_env.name}\")\n",
    "print(f\"Experiment: {exp.name}\")\n",
    "\n",
    "config = ScriptRunConfig(\n",
    "    source_directory = str(project_dir), \n",
    "    script = \"scripts/vae/train.py\",\n",
    "    arguments=helpers.argsdict2list(args),\n",
    ")\n",
    "config.run_config = run_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config.run_config.target = \"nc24-uswest2\"\n",
    "config.run_config.target = \"NC24rs-v3-usw2-d\"\n",
    "\n",
    "run = exp.submit(config)\n",
    "display(Markdown(f\"\"\"\n",
    "- Environment: {pytorch_env.name}\n",
    "- Experiment: [{run.experiment.name}]({run.experiment.get_portal_url()})\n",
    "- Run: [{run.display_name}]({run.get_portal_url()})\n",
    "- Target: {config.run_config.target}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain hyper drive\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args:\n",
      " accelerator: ddp\n",
      " auto_lr_find: False\n",
      " auto_scale_batch_size: False\n",
      " base_lr: ['choice', [[0.0001, 1e-05, 1e-06]]]\n",
      " batch_size: ['choice', [[16, 32]]]\n",
      " channels: 1\n",
      " data_folder: <azureml.data.dataset_consumption_config.DatasetConsumptionConfig object at 0x000002A1052BED68>\n",
      " dataset: chexpert\n",
      " flush_logs_every_n_steps: 25\n",
      " frontal_only: 0\n",
      " ignore_nonfrontal_loss: 0\n",
      " image_size: ['choice', [[128, 256]]]\n",
      " layer_count: ['choice', [[3, 4]]]\n",
      " log_every_n_steps: 25\n",
      " log_recon_images: 32\n",
      " lr_scheduler: plateau\n",
      " max_epochs: 50\n",
      " normalize: False\n",
      " num_workers: -1\n",
      " output_dir: ./outputs\n",
      " progress_bar_refresh_rate: 25\n",
      " run_azure: 1\n",
      " step_size: 3\n",
      " terminate_on_nan: True\n",
      " val_frontal_only: 0\n",
      " width: ['choice', [[160, 240, 320]]]\n",
      " z: ['choice', [[32, 64, 128]]]\n",
      "Environment: vae\n",
      "Experiment: vae-chexpert-tune\n"
     ]
    }
   ],
   "source": [
    "from azureml.train.hyperdrive import GridParameterSampling, RandomParameterSampling, BanditPolicy, HyperDriveConfig, uniform, PrimaryMetricGoal, choice, loguniform\n",
    "run_config = RunConfiguration()\n",
    "\n",
    "cluster_name = \"nc24-uswest2\"\n",
    "# cluster_name = \"NC24rs-v3-usw2-d\"\n",
    "\n",
    "\n",
    "run_config.environment = pytorch_env\n",
    "run_config.docker = DockerConfiguration(use_docker=True, shm_size=\"100G\")\n",
    "run_config.target = cluster_name\n",
    "\n",
    "\n",
    "param_sampling = RandomParameterSampling(\n",
    "    {\n",
    "        \"layer_count\": choice(3, 4),\n",
    "        \"batch_size\": choice(16, 32),\n",
    "        \"image_size\": choice(128, 256),\n",
    "        \"z\": choice(32, 64, 128),\n",
    "        \"width\": choice(160, 240, 320),\n",
    "        \"base_lr\": choice(1e-4, 1e-5, 1e-6),\n",
    "        \"kl_coeff\": choice(0.1)\n",
    "    }\n",
    ")\n",
    "\n",
    "experiment_name = f'vae-{dataset_name}-tune'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)\n",
    "config.run_config = run_config\n",
    "\n",
    "from azureml.train.hyperdrive import BanditPolicy\n",
    "early_termination_policy = BanditPolicy(slack_factor = 0.2, evaluation_interval=1, delay_evaluation=10)\n",
    "\n",
    "hyperdrive_config = HyperDriveConfig(run_config=config,\n",
    "                                     hyperparameter_sampling=param_sampling, \n",
    "                                     policy=early_termination_policy,\n",
    "                                     primary_metric_name='val/weighted_recon_loss',\n",
    "                                     primary_metric_goal=PrimaryMetricGoal.MINIMIZE,\n",
    "                                     max_total_runs=6*12,\n",
    "                                     max_concurrent_runs=4)\n",
    "\n",
    "\n",
    "print(\"args:\")\n",
    "for k,v in sorted(zip(config.arguments[::2], config.arguments[1::2])):\n",
    "    k = k.strip(\"-\")\n",
    "    v = param_sampling._parameter_space.get(k, v)\n",
    "    print(f\" {k}: {v}\")\n",
    "\n",
    "print(f\"Environment: {pytorch_env.name}\")\n",
    "print(f\"Experiment: {exp.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "- Experiement: [vae-chexpert-tune](https://ml.azure.com/experiments/vae-chexpert-tune?wsid=/subscriptions/9ca8df1a-bf40-49c6-a13f-66b72a85f43c/resourcegroups/MLOps-Prototype/workspaces/MLOps_shared&tid=72f988bf-86f1-41af-91ab-2d7cd011db47)\n",
       "- Run: [dynamic_holiday_yv5gfkcx](https://ml.azure.com/runs/HD_68c11bd0-297f-4ad1-9902-3ba5c9017e12?wsid=/subscriptions/9ca8df1a-bf40-49c6-a13f-66b72a85f43c/resourcegroups/MLOps-Prototype/workspaces/MLOps_shared&tid=72f988bf-86f1-41af-91ab-2d7cd011db47)\n",
       "- Target: nc24-uswest2\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start the HyperDrive run\n",
    "hyperdrive_run = exp.submit(hyperdrive_config)\n",
    "display(Markdown(f\"\"\"\n",
    "- Experiement: [{hyperdrive_run.experiment.name}]({hyperdrive_run.experiment.get_portal_url()})\n",
    "- Run: [{hyperdrive_run.display_name}]({hyperdrive_run.get_portal_url()})\n",
    "- Target: {config.run_config.target}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40209cfd1e49aba1e20a3908f9a243f43b2ed73034fd3a81730d62124bbdcdae"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
