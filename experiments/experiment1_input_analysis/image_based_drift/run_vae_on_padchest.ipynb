{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (cloudpickle 2.0.0 (d:\\code\\mlopsday2\\medimaging-modeldriftmonitoring\\.venv\\lib\\site-packages), Requirement.parse('cloudpickle<2.0.0,>=1.1.0'), {'azureml-dataprep'}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML SDK Version:  1.34.0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import azureml\n",
    "from IPython.display import display, Markdown\n",
    "from azureml.core import Run, Model\n",
    "from azureml.core import Datastore, Experiment, ScriptRunConfig, Workspace, RunConfiguration\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.runconfig import DockerConfiguration\n",
    "from azureml.exceptions import UserErrorException\n",
    "\n",
    "from model_drift import settings\n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library configuration succeeded\n",
      "Workspace: MLOps_shared\n"
     ]
    }
   ],
   "source": [
    "# Connect to workspace\n",
    "subscription_id = '9ca8df1a-bf40-49c6-a13f-66b72a85f43c'\n",
    "resource_group = 'MLOps-Prototype'\n",
    "workspace_name = 'MLOps_shared'\n",
    "\n",
    "try:\n",
    "    ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
    "    ws.write_config()\n",
    "    print('Library configuration succeeded')\n",
    "except:\n",
    "    print('Workspace not found')\n",
    "\n",
    "print(\"Workspace:\", ws.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Incorporate model and model registration into notebook\n",
    "\n",
    "# experiment_name = 'chexpert-vae-tune'\n",
    "# exp = Experiment(workspace=ws, name=experiment_name)\n",
    "# for r in exp.get_runs():\n",
    "#     if r.display_name  == \"plum_loquat_9kq2qqtn\":\n",
    "#         break\n",
    "# for run in r.get_children():\n",
    "#     if run.display_name == \"wheat_book_mmkzkspg\":\n",
    "#         break\n",
    "\n",
    "# from azureml.core.resource_configuration import ResourceConfiguration\n",
    "\n",
    "# model = run.register_model(model_name='chexpert-vae-wheat_book_mmkzkspg-049',\n",
    "#                            model_path='outputs/checkpoints/last.ckpt/049.ckpt',\n",
    "#                            model_framework='PyTorch',\n",
    "#                            model_framework_version='1.8',\n",
    "#                            description=\"TBD\",\n",
    "#                            resource_configuration=ResourceConfiguration(cpu=8, memory_in_gb=2, gpu=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Dataset Name: chexpert_vae_wheat_book_mmkzkspg_049_dbg\n",
      "Experiment: chexpert-vae-OnPadChest-dbg\n",
      "Environment: vae\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "- Experiement: [chexpert-vae-OnPadChest-dbg](https://ml.azure.com/experiments/chexpert-vae-OnPadChest-dbg?wsid=/subscriptions/9ca8df1a-bf40-49c6-a13f-66b72a85f43c/resourcegroups/MLOps-Prototype/workspaces/MLOps_shared&tid=72f988bf-86f1-41af-91ab-2d7cd011db47)\n",
       "- Run: [shy_dinner_p1mj4c8c](https://ml.azure.com/runs/chexpert-vae-OnPadChest-dbg_1633632632_5c17a264?wsid=/subscriptions/9ca8df1a-bf40-49c6-a13f-66b72a85f43c/resourcegroups/MLOps-Prototype/workspaces/MLOps_shared&tid=72f988bf-86f1-41af-91ab-2d7cd011db47)\n",
       "- Target: NC24rs-v3-usw2-d\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "dbg = True\n",
    "log_refresh_rate = 1\n",
    "\n",
    "# Name experiement\n",
    "input_dataset_name=\"padchest\"\n",
    "experiment_name = 'chexpert-vae-OnPadChest-dbg'\n",
    "model_name =  \"chexpert-vae-wheat_book_mmkzkspg-049\"\n",
    "datastore_name = \"vaeresults_padchest\"\n",
    "env_name = \"vae\"\n",
    "compute_target = \"nc6-uswest2\"\n",
    "compute_target = \"NC24rs-v3-usw2-d\"\n",
    "\n",
    "\n",
    "exp = Experiment(workspace=ws, name=experiment_name)\n",
    "environment_file = settings.CONDA_ENVIRONMENT_FILE\n",
    "project_dir = Path(\"./experiment\")\n",
    "pytorch_env = Environment.from_conda_specification(env_name, file_path =str(environment_file))\n",
    "\n",
    "pytorch_env.register(workspace=ws)\n",
    "build = pytorch_env.build(workspace=ws)\n",
    "pytorch_env.environment_variables[\"RSLEX_DIRECT_VOLUME_MOUNT\"] = \"True\"\n",
    "pc_dataset = Dataset.get_by_name(ws, name=input_dataset_name)\n",
    "datastore = ws.datastores[datastore_name]\n",
    "output_dataset_name = re.sub(r'\\W+', \"\" ,f\"{model_name}\".replace('-', \"_\"))\n",
    "output_dir =  f\"{output_dataset_name}\"\n",
    "\n",
    "if dbg:\n",
    "    output_dataset_name = f\"{output_dataset_name}_dbg\"\n",
    "    output_dir = f\"{output_dir}/dbg\"\n",
    "\n",
    "output_dir = output_dir.replace(\"//\", \"/\")\n",
    "output_dataset = OutputFileDatasetConfig(name=output_dataset_name, destination=(datastore, output_dir.strip(\"/\")+\"/\"))\n",
    "\n",
    "print(\"Output Dataset Name:\", output_dataset_name)\n",
    "print(\"Experiment:\", exp.name)\n",
    "print(\"Environment:\", pytorch_env.name)\n",
    "\n",
    "run_config = RunConfiguration()\n",
    "run_config.environment = pytorch_env\n",
    "run_config.docker = DockerConfiguration(use_docker=True, shm_size=\"100G\")\n",
    "\n",
    "run_config.output_data = {\"output_dataset_name\": output_dataset}\n",
    "\n",
    "\n",
    "args = [\n",
    "    \"--run_azure\", 1,\n",
    "    \"--model\", model_name,\n",
    "    '--data_folder', pc_dataset.as_named_input('padchestv1').as_mount(),\n",
    "    '--progress_bar_refresh_rate', log_refresh_rate,\n",
    "    \"--log_every_n_steps\", log_refresh_rate,\n",
    "    \"--flush_logs_every_n_steps\", log_refresh_rate,\n",
    "    \"--batch_size\", 128,\n",
    "    \"--accelerator\", \"ddp\",\n",
    "    \"--output_dir\", output_dataset.as_mount(),\n",
    "    \"--write_recon\", 1,\n",
    "    \"--write_grid\",  1,\n",
    "    \"--latent_output_dir\", \"./outputs/\",\n",
    "    \"--append_run_name\", 1,\n",
    "    ]\n",
    "\n",
    "if dbg:\n",
    "    args += [\n",
    "        '--limit_predict_batches', 10,\n",
    "    ]\n",
    "\n",
    "\n",
    "config = ScriptRunConfig(\n",
    "    source_directory = str(project_dir), \n",
    "    script = \"score.py\",\n",
    "    arguments=args,\n",
    ")\n",
    "run_config.target = compute_target\n",
    "config.run_config = run_config\n",
    "\n",
    "run = exp.submit(config)\n",
    "display(Markdown(f\"\"\"\n",
    "- Experiement: [{run.experiment.name}]({run.experiment.get_portal_url()})\n",
    "- Run: [{run.display_name}]({run.get_portal_url()})\n",
    "- Target: {config.run_config.target}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: dataset_debug\n",
      "Environment: vae\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "- Experiement: [dataset_debug](https://ml.azure.com/experiments/dataset_debug?wsid=/subscriptions/9ca8df1a-bf40-49c6-a13f-66b72a85f43c/resourcegroups/MLOps-Prototype/workspaces/MLOps_shared&tid=72f988bf-86f1-41af-91ab-2d7cd011db47)\n",
       "- Run: [dreamy_school_t7mkvb9q](https://ml.azure.com/runs/dataset_debug_1633542663_b1d850e4?wsid=/subscriptions/9ca8df1a-bf40-49c6-a13f-66b72a85f43c/resourcegroups/MLOps-Prototype/workspaces/MLOps_shared&tid=72f988bf-86f1-41af-91ab-2d7cd011db47)\n",
       "- Target: nc6-uswest2\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Move to another notebook or delete?\n",
    "\n",
    "# experiment_name = \"dataset_debug\"\n",
    "# chex_dataset = Dataset.get_by_name(ws, name='chexpert')\n",
    "# pc_dataset = Dataset.get_by_name(ws, name=input_dataset_name)\n",
    "\n",
    "\n",
    "# exp = Experiment(workspace=ws, name=experiment_name)\n",
    "# environment_file = settings.CONDA_ENVIRONMENT_FILE\n",
    "# project_dir = Path(\"./experiment\")\n",
    "# pytorch_env = Environment.from_conda_specification(env_name, file_path =str(environment_file))\n",
    "\n",
    "# pytorch_env.register(workspace=ws)\n",
    "# build = pytorch_env.build(workspace=ws)\n",
    "\n",
    "# pytorch_env.environment_variables[\"RSLEX_DIRECT_VOLUME_MOUNT\"] = \"True\"\n",
    "\n",
    "# print(\"Experiment:\", exp.name)\n",
    "# print(\"Environment:\", pytorch_env.name)\n",
    "\n",
    "# run_config = RunConfiguration()\n",
    "# run_config.environment = pytorch_env\n",
    "# run_config.docker = DockerConfiguration(use_docker=True, shm_size=\"100G\")\n",
    "\n",
    "# args = [\n",
    "#     '--data1', pc_dataset.as_named_input('padchestv1').as_mount(),\n",
    "#     '--data2', chex_dataset.as_named_input('chexpertv1').as_mount(),\n",
    "#     ]\n",
    "\n",
    "\n",
    "# config = ScriptRunConfig(\n",
    "#     source_directory = str(project_dir), \n",
    "#     script = \"dataset_test.py\",\n",
    "#     arguments=args,\n",
    "# )\n",
    "\n",
    "# config.run_config = run_config\n",
    "\n",
    "# config.run_config.target = compute_target\n",
    "\n",
    "# # config.run_config.target = \"nc6-uswest2\"\n",
    "\n",
    "# run = exp.submit(config)\n",
    "# display(Markdown(f\"\"\"\n",
    "# - Experiement: [{run.experiment.name}]({run.experiment.get_portal_url()})\n",
    "# - Run: [{run.display_name}]({run.get_portal_url()})\n",
    "# - Target: {config.run_config.target}\n",
    "# \"\"\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40209cfd1e49aba1e20a3908f9a243f43b2ed73034fd3a81730d62124bbdcdae"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
