{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (cloudpickle 2.0.0 (d:\\code\\mlopsday2\\medimaging-modeldriftmonitoring\\.venv\\lib\\site-packages), Requirement.parse('cloudpickle<2.0.0,>=1.1.0'), {'azureml-dataprep'}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML SDK Version:  1.35.0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import azureml\n",
    "from IPython.display import display, Markdown\n",
    "from azureml.core import Datastore, Experiment, ScriptRunConfig, Workspace, RunConfiguration\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.runconfig import DockerConfiguration\n",
    "from azureml.exceptions import UserErrorException\n",
    "import shutil\n",
    "\n",
    "\n",
    "from model_drift import settings, helpers\n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    }
   ],
   "source": [
    "# Connect to workspace\n",
    "ws = Workspace.from_config(settings.AZUREML_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_folder <azureml.data.dataset_consumption_config.DatasetConsumptionConfig object at 0x000001B68A17ABA8>\n",
      "run_azure 1\n",
      "batch_size 6\n",
      "output_dir ./outputs\n",
      "pretrained pretrained-chexpert/iter_662400.pth.tar\n",
      "num_workers -1\n",
      "max_epochs 5\n",
      "progress_bar_refresh_rate 1\n",
      "log_every_n_steps 1\n",
      "flush_logs_every_n_steps 1\n",
      "accelerator ddp\n",
      "freeze_backbone 0\n",
      "frontal_only 1\n",
      "num_sanity_val_steps 40\n",
      "limit_train_batches 5\n",
      "limit_val_batches 40\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'run' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_47956/2846834016.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mRun\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay_name\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_portal_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mTarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m \"\"\"))\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'run' is not defined"
     ]
    }
   ],
   "source": [
    "dbg = True\n",
    "\n",
    "log_refresh_rate = 25\n",
    "if dbg:\n",
    "    log_refresh_rate = 1\n",
    "\n",
    "env_name = \"finetune-padchest\"\n",
    "\n",
    "# Name experiement\n",
    "experiment_name = 'finetune-padchest' if not dbg else 'finetune-padchest-dbg'\n",
    "\n",
    "# Input Dataset\n",
    "dataset = Dataset.get_by_name(ws, name='padchest')\n",
    "\n",
    "#Experiment\n",
    "exp = Experiment(workspace=ws, name=experiment_name)\n",
    "\n",
    "#Environment\n",
    "environment_file = settings.CONDA_ENVIRONMENT_FILE\n",
    "project_dir = settings.SRC_DIR\n",
    "pytorch_env = Environment.from_conda_specification(env_name, file_path =str(environment_file))\n",
    "pytorch_env.register(workspace=ws)\n",
    "build = pytorch_env.build(workspace=ws)\n",
    "pytorch_env.environment_variables[\"RSLEX_DIRECT_VOLUME_MOUNT\"] = \"True\"\n",
    "\n",
    "# Run Configuration\n",
    "run_config = RunConfiguration()\n",
    "run_config.environment_variables[\"RSLEX_DIRECT_VOLUME_MOUNT\"] = \"True\"\n",
    "run_config.environment = pytorch_env\n",
    "run_config.docker = DockerConfiguration(use_docker=True, shm_size=\"100G\")\n",
    "\n",
    "\n",
    "args = {\n",
    " 'data_folder': dataset.as_named_input('dataset').as_mount(),\n",
    " 'run_azure': 1,\n",
    " 'batch_size': 6,\n",
    " 'output_dir': './outputs',\n",
    " 'pretrained': 'pretrained-chexpert/iter_662400.pth.tar',\n",
    " 'num_workers': -1,\n",
    " 'max_epochs':  24 if not dbg else 5,\n",
    " 'progress_bar_refresh_rate': 25 if not dbg else 1,\n",
    " 'log_every_n_steps':  25 if not dbg else 1,\n",
    " 'flush_logs_every_n_steps':  25 if not dbg else 1,\n",
    " 'accelerator': 'ddp',\n",
    " 'freeze_backbone': 0,\n",
    " 'frontal_only': 1,\n",
    " 'num_sanity_val_steps': 0\n",
    " }\n",
    "\n",
    "if dbg:\n",
    "    args.update({\n",
    "        'limit_train_batches': 5,\n",
    "        'limit_val_batches': 40,\n",
    "        \"num_sanity_val_steps\": 40\n",
    "    })\n",
    "\n",
    "args.setdefault(\"num_sanity_val_steps\", 0)\n",
    "\n",
    "\n",
    "for param, value in args.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "\n",
    "config = ScriptRunConfig(\n",
    "    source_directory = str(project_dir), \n",
    "    script = \"scripts/finetune/train.py\",\n",
    "    arguments=helpers.argsdict2list(args),\n",
    ")\n",
    "\n",
    "config.run_config = run_config\n",
    "\n",
    "for param, value in args.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.run_config.target = \"nc24-uswest2\"\n",
    "# config.run_config.target = \"NC24rs-v3-usw2-d\"\n",
    "run = exp.submit(config)\n",
    "display(Markdown(f\"\"\"\n",
    "- Environment: {pytorch_env.name}\n",
    "- Experiment: [{run.experiment.name}]({run.experiment.get_portal_url()})\n",
    "- Run: [{run.display_name}]({run.get_portal_url()})\n",
    "- Target: {config.run_config.target}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_folder <azureml.data.dataset_consumption_config.DatasetConsumptionConfig object at 0x000001B6FFF51550>\n",
      "run_azure 1\n",
      "batch_size 6\n",
      "output_dir ./outputs\n",
      "pretrained pretrained-chexpert/iter_662400.pth.tar\n",
      "num_workers -1\n",
      "max_epochs 24\n",
      "progress_bar_refresh_rate 25\n",
      "log_every_n_steps 25\n",
      "flush_logs_every_n_steps 25\n",
      "accelerator ddp\n",
      "freeze_backbone 0\n",
      "frontal_only 1\n",
      "num_sanity_val_steps 0\n"
     ]
    }
   ],
   "source": [
    "from azureml.train.hyperdrive import GridParameterSampling, RandomParameterSampling, BanditPolicy, HyperDriveConfig, uniform, PrimaryMetricGoal, choice, loguniform\n",
    "run_config = RunConfiguration()\n",
    "\n",
    "cluster_name = \"NC24rs-v3-usw2-d\"\n",
    "# cluster_name = \"NC24rs-v3-usw2-l\"\n",
    "\n",
    "run_config.environment = pytorch_env\n",
    "run_config.docker = DockerConfiguration(use_docker=True, shm_size=\"100G\")\n",
    "run_config.target = cluster_name\n",
    "\n",
    "if \"limit_train_batches\" in args:\n",
    "    del args[\"limit_train_batches\"]\n",
    "\n",
    "if \"limit_val_batches\" in args:\n",
    "    del args[\"limit_val_batches\"]\n",
    "\n",
    "args.update({\n",
    " 'max_epochs':  24,\n",
    " 'progress_bar_refresh_rate': 25,\n",
    " 'log_every_n_steps':  25,\n",
    " 'flush_logs_every_n_steps':  25,\n",
    " \"num_sanity_val_steps\": 0\n",
    "})\n",
    "\n",
    "\n",
    "param_sampling = RandomParameterSampling(\n",
    "    {   \"freeze_backbone\": choice([0, 1]),\n",
    "        \"batch_size\": choice([8, 12, 16]),\n",
    "        \"learning_rate\": choice(1e-2, 1e-4, 1e-4),\n",
    "        \"step_size\": choice([4, 8, 12]),\n",
    "        \"accumulate_grad_batches\": choice([1, 2, 4]),\n",
    "        \"frontal_only\": choice([0, 1])\n",
    "    }\n",
    ")\n",
    "\n",
    "experiment_name = 'finetune-padchest-hyper'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)\n",
    "config.run_config = run_config\n",
    "hyperdrive_config = HyperDriveConfig(run_config=config,\n",
    "                                     hyperparameter_sampling=param_sampling, \n",
    "                                     policy=None,\n",
    "                                     primary_metric_name='val/AUROC.mean',\n",
    "                                     primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                                     max_total_runs=6*18,\n",
    "                                     max_concurrent_runs=6)\n",
    "\n",
    "for param, value in args.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the HyperDrive run\n",
    "hyperdrive_run = exp.submit(hyperdrive_config)\n",
    "display(Markdown(f\"\"\"\n",
    "- Experiment: [{hyperdrive_run.experiment.name}]({hyperdrive_run.experiment.get_portal_url()})\n",
    "- Run: [{hyperdrive_run.display_name}]({hyperdrive_run.get_portal_url()})\n",
    "- Target: {config.run_config.target}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40209cfd1e49aba1e20a3908f9a243f43b2ed73034fd3a81730d62124bbdcdae"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
