{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (cloudpickle 2.0.0 (d:\\code\\mlopsday2\\medimaging-modeldriftmonitoring\\.venv\\lib\\site-packages), Requirement.parse('cloudpickle<2.0.0,>=1.1.0'), {'azureml-dataprep'}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML SDK Version:  1.34.0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import azureml\n",
    "from IPython.display import display, Markdown\n",
    "from azureml.core import Datastore, Experiment, ScriptRunConfig, Workspace, RunConfiguration\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.runconfig import DockerConfiguration\n",
    "from azureml.exceptions import UserErrorException\n",
    "import shutil\n",
    "\n",
    "\n",
    "from model_drift import settings, helpers\n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    }
   ],
   "source": [
    "# Connect to workspace\n",
    "subscription_id = '9ca8df1a-bf40-49c6-a13f-66b72a85f43c'\n",
    "resource_group = 'MLOps-Prototype'\n",
    "workspace_name = 'MLOps_shared'\n",
    "\n",
    "ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Dataset\n",
    "pc_dataset = Dataset.get_by_name(ws, name='padchest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"finetune-padchest\"\n",
    "\n",
    "environment_file = settings.CONDA_ENVIRONMENT_FILE\n",
    "project_dir = Path(\"./experiment/\")\n",
    "\n",
    "helpers.copytree(settings.TOP_DIR.joinpath('model_drift'), project_dir.joinpath('model_drift'))\n",
    "\n",
    "pytorch_env = Environment.from_conda_specification(env_name, file_path =str(environment_file))\n",
    "\n",
    "pytorch_env.register(workspace=ws)\n",
    "build = pytorch_env.build(workspace=ws)\n",
    "\n",
    "pytorch_env.environment_variables[\"RSLEX_DIRECT_VOLUME_MOUNT\"] = \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: finetune-padchest\n",
      "Environment: finetune-padchest\n"
     ]
    }
   ],
   "source": [
    "dbg = False\n",
    "\n",
    "helpers.copytree(settings.TOP_DIR.joinpath('model_drift'), project_dir.joinpath('model_drift'))\n",
    "\n",
    "log_refresh_rate = 25\n",
    "if dbg:\n",
    "    log_refresh_rate = 1\n",
    "\n",
    "# Name experiement\n",
    "experiment_name = 'finetune-padchest' if not dbg else 'finetune-padchest-dbg'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)\n",
    "\n",
    "print(\"Experiment:\", exp.name)\n",
    "print(\"Environment:\", pytorch_env.name)\n",
    "\n",
    "run_config = RunConfiguration()\n",
    "\n",
    "run_config.environment = pytorch_env\n",
    "run_config.docker = DockerConfiguration(use_docker=True, shm_size=\"100G\")\n",
    "args = [\n",
    "    '--data_folder', pc_dataset.as_named_input('padchestv1').as_mount(),\n",
    "    '--csv_root', \".\",\n",
    "    '--run_azure', 1,\n",
    "    \"--batch_size\", 6,\n",
    "    '--output_dir', './outputs',\n",
    "    '--train_csv', \"padchest_10labels_train.csv\",\n",
    "    \"--val_csv\", \"padchest_10labels_val.csv\",\n",
    "    \"--checkpoint\", \"pretrained-chexpert-iter_662400.pth.tar\",\n",
    "\n",
    "    '--max_epochs', 24 if not dbg else 5,\n",
    "    '--num_workers', -1,\n",
    "\n",
    "    '--progress_bar_refresh_rate', log_refresh_rate,\n",
    "    \"--log_every_n_steps\", log_refresh_rate,\n",
    "    \"--flush_logs_every_n_steps\", log_refresh_rate,\n",
    "    \"--accelerator\", \"ddp\",\n",
    "    \"--freeze_backbone\", 0,\n",
    "\n",
    "    ]\n",
    "\n",
    "if dbg:\n",
    "    args += [\n",
    "        # '--limit_train_batches', 5,\n",
    "        # '--limit_val_batches', 40,\n",
    "        # \"--num_sanity_val_steps\", 40\n",
    "    ]\n",
    "\n",
    "if \"--num_sanity_val_steps\" not in args:\n",
    "    args += [\"--num_sanity_val_steps\", 0]\n",
    "\n",
    "config = ScriptRunConfig(\n",
    "    source_directory = str(project_dir), \n",
    "    script = \"train.py\",\n",
    "    arguments=args,\n",
    ")\n",
    "\n",
    "config.run_config = run_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "- Experiement: [finetune-padchest](https://ml.azure.com/experiments/finetune-padchest?wsid=/subscriptions/9ca8df1a-bf40-49c6-a13f-66b72a85f43c/resourcegroups/MLOps-Prototype/workspaces/MLOps_shared&tid=72f988bf-86f1-41af-91ab-2d7cd011db47)\n",
       "- Run: [bright_loquat_47w5k3hp](https://ml.azure.com/runs/finetune-padchest_1635021703_36eaf995?wsid=/subscriptions/9ca8df1a-bf40-49c6-a13f-66b72a85f43c/resourcegroups/MLOps-Prototype/workspaces/MLOps_shared&tid=72f988bf-86f1-41af-91ab-2d7cd011db47)\n",
       "- Target: nc24-uswest2\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config.run_config.target = \"nc24-uswest2\"\n",
    "# config.run_config.target = \"NC24rs-v3-usw2-d\"\n",
    "run = exp.submit(config)\n",
    "display(Markdown(f\"\"\"\n",
    "- Experiement: [{run.experiment.name}]({run.experiment.get_portal_url()})\n",
    "- Run: [{run.display_name}]({run.get_portal_url()})\n",
    "- Target: {config.run_config.target}\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import GridParameterSampling, RandomParameterSampling, BanditPolicy, HyperDriveConfig, uniform, PrimaryMetricGoal, choice, loguniform\n",
    "run_config = RunConfiguration()\n",
    "\n",
    "cluster_name = \"NC24rs-v3-usw2-d\"\n",
    "# cluster_name = \"nc24-uswest2\"\n",
    "\n",
    "run_config.environment = pytorch_env\n",
    "run_config.docker = DockerConfiguration(use_docker=True, shm_size=\"100G\")\n",
    "run_config.target = cluster_name\n",
    "\n",
    "\n",
    "param_sampling = RandomParameterSampling(\n",
    "    {   \"freeze_backbone\": choice([0, 1]),\n",
    "        \"batch_size\": choice([8, 12, 16]),\n",
    "        \"learning_rate\": choice(1e-2, 1e-4, 1e-4),\n",
    "        \"step_size\": choice([4, 8, 12]),\n",
    "        \"accumulate_grad_batches\": choice([1, 2, 4])\n",
    "    }\n",
    ")\n",
    "\n",
    "experiment_name = 'finetune-padchest-hyper'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)\n",
    "config.run_config = run_config\n",
    "hyperdrive_config = HyperDriveConfig(run_config=config,\n",
    "                                     hyperparameter_sampling=param_sampling, \n",
    "                                     policy=None,\n",
    "                                     primary_metric_name='val/loss',\n",
    "                                     primary_metric_goal=PrimaryMetricGoal.MINIMIZE,\n",
    "                                     max_total_runs=6*12,\n",
    "                                     max_concurrent_runs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "- Experiment: [finetune-padchest-hyper](https://ml.azure.com/experiments/finetune-padchest-hyper?wsid=/subscriptions/9ca8df1a-bf40-49c6-a13f-66b72a85f43c/resourcegroups/MLOps-Prototype/workspaces/MLOps_shared&tid=72f988bf-86f1-41af-91ab-2d7cd011db47)\n",
       "- Run: [zen_net_t30h0y0w](https://ml.azure.com/runs/HD_e9329a40-b7e2-4309-b35a-3ed45c4de352?wsid=/subscriptions/9ca8df1a-bf40-49c6-a13f-66b72a85f43c/resourcegroups/MLOps-Prototype/workspaces/MLOps_shared&tid=72f988bf-86f1-41af-91ab-2d7cd011db47)\n",
       "- Target: NC24rs-v3-usw2-d\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start the HyperDrive run\n",
    "hyperdrive_run = exp.submit(hyperdrive_config)\n",
    "display(Markdown(f\"\"\"\n",
    "- Experiment: [{hyperdrive_run.experiment.name}]({hyperdrive_run.experiment.get_portal_url()})\n",
    "- Run: [{hyperdrive_run.display_name}]({hyperdrive_run.get_portal_url()})\n",
    "- Target: {config.run_config.target}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====\n",
      " Pytorch Lightning Version: 1.4.7"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (cloudpickle 2.0.0 (d:\\code\\mlopsday2\\medimaging-modeldriftmonitoring\\.venv\\lib\\site-packages), Requirement.parse('cloudpickle<2.0.0,>=1.1.0'), {'azureml-dataprep'}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Pytorch Version: 1.8.0\n",
      " Num GPUs: 1\n",
      " Num CPUs: 16\n",
      " Node Rank: 0\n",
      " Local Rank: 0\n",
      " World Size: 1\n",
      " Global Rank: 0\n",
      " Rank ID: 0-0\n",
      "=====\n",
      "\n",
      "usage: train.py [-h] [--run_azure RUN_AZURE] [--output_dir OUTPUT_DIR]\n",
      "                [--data_folder DATA_FOLDER] [--batch_size BATCH_SIZE]\n",
      "                [--train_csv TRAIN_CSV] [--val_csv VAL_CSV]\n",
      "                [--checkpoint CHECKPOINT] [--csv_root CSV_ROOT]\n",
      "                [--num_classes NUM_CLASSES] [--num_workers NUM_WORKERS]\n",
      "                [--learning_rate LEARNING_RATE]\n",
      "                [--freeze_backbone FREEZE_BACKBONE]\n",
      "                [--weight_decay WEIGHT_DECAY] [--gamma GAMMA]\n",
      "                [--step_size STEP_SIZE] [--logger [LOGGER]]\n",
      "                [--checkpoint_callback [CHECKPOINT_CALLBACK]]\n",
      "                [--default_root_dir DEFAULT_ROOT_DIR]\n",
      "                [--gradient_clip_val GRADIENT_CLIP_VAL]\n",
      "                [--gradient_clip_algorithm GRADIENT_CLIP_ALGORITHM]\n",
      "                [--process_position PROCESS_POSITION] [--num_nodes NUM_NODES]\n",
      "                [--num_processes NUM_PROCESSES] [--devices DEVICES]\n",
      "                [--gpus GPUS] [--auto_select_gpus [AUTO_SELECT_GPUS]]\n",
      "                [--tpu_cores TPU_CORES] [--ipus IPUS]\n",
      "                [--log_gpu_memory LOG_GPU_MEMORY]\n",
      "                [--progress_bar_refresh_rate PROGRESS_BAR_REFRESH_RATE]\n",
      "                [--overfit_batches OVERFIT_BATCHES]\n",
      "                [--track_grad_norm TRACK_GRAD_NORM]\n",
      "                [--check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH]\n",
      "                [--fast_dev_run [FAST_DEV_RUN]]\n",
      "                [--accumulate_grad_batches ACCUMULATE_GRAD_BATCHES]\n",
      "                [--max_epochs MAX_EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                [--max_steps MAX_STEPS] [--min_steps MIN_STEPS]\n",
      "                [--max_time MAX_TIME]\n",
      "                [--limit_train_batches LIMIT_TRAIN_BATCHES]\n",
      "                [--limit_val_batches LIMIT_VAL_BATCHES]\n",
      "                [--limit_test_batches LIMIT_TEST_BATCHES]\n",
      "                [--limit_predict_batches LIMIT_PREDICT_BATCHES]\n",
      "                [--val_check_interval VAL_CHECK_INTERVAL]\n",
      "                [--flush_logs_every_n_steps FLUSH_LOGS_EVERY_N_STEPS]\n",
      "                [--log_every_n_steps LOG_EVERY_N_STEPS]\n",
      "                [--accelerator ACCELERATOR]\n",
      "                [--sync_batchnorm [SYNC_BATCHNORM]] [--precision PRECISION]\n",
      "                [--weights_summary WEIGHTS_SUMMARY]\n",
      "                [--weights_save_path WEIGHTS_SAVE_PATH]\n",
      "                [--num_sanity_val_steps NUM_SANITY_VAL_STEPS]\n",
      "                [--truncated_bptt_steps TRUNCATED_BPTT_STEPS]\n",
      "                [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
      "                [--profiler PROFILER] [--benchmark [BENCHMARK]]\n",
      "                [--deterministic [DETERMINISTIC]]\n",
      "                [--reload_dataloaders_every_n_epochs RELOAD_DATALOADERS_EVERY_N_EPOCHS]\n",
      "                [--reload_dataloaders_every_epoch [RELOAD_DATALOADERS_EVERY_EPOCH]]\n",
      "                [--auto_lr_find [AUTO_LR_FIND]]\n",
      "                [--replace_sampler_ddp [REPLACE_SAMPLER_DDP]]\n",
      "                [--terminate_on_nan [TERMINATE_ON_NAN]]\n",
      "                [--auto_scale_batch_size [AUTO_SCALE_BATCH_SIZE]]\n",
      "                [--prepare_data_per_node [PREPARE_DATA_PER_NODE]]\n",
      "                [--plugins PLUGINS] [--amp_backend AMP_BACKEND]\n",
      "                [--amp_level AMP_LEVEL]\n",
      "                [--distributed_backend DISTRIBUTED_BACKEND]\n",
      "                [--move_metrics_to_cpu [MOVE_METRICS_TO_CPU]]\n",
      "                [--multiple_trainloader_mode MULTIPLE_TRAINLOADER_MODE]\n",
      "                [--stochastic_weight_avg [STOCHASTIC_WEIGHT_AVG]]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --run_azure RUN_AZURE\n",
      "                        run in AzureML\n",
      "  --output_dir OUTPUT_DIR\n",
      "                        output directory\n",
      "\n",
      "module:\n",
      "  --data_folder DATA_FOLDER\n",
      "                        data folder mounting point\n",
      "  --batch_size BATCH_SIZE\n",
      "                        batch size\n",
      "  --train_csv TRAIN_CSV\n",
      "                        train csv filename\n",
      "  --val_csv VAL_CSV     validation data csv\n",
      "  --checkpoint CHECKPOINT\n",
      "                        checkpoint to fine tune from\n",
      "  --csv_root CSV_ROOT   csv_root\n",
      "  --num_classes NUM_CLASSES\n",
      "                        number of output classes\n",
      "  --num_workers NUM_WORKERS\n",
      "                        number of workers for data loading\n",
      "  --learning_rate LEARNING_RATE\n",
      "                        base learning rate\n",
      "  --freeze_backbone FREEZE_BACKBONE\n",
      "                        freeze_backbone\n",
      "  --weight_decay WEIGHT_DECAY\n",
      "                        weight decay for optimizer\n",
      "  --gamma GAMMA         reduction factor for lr scheduler. if reduce on\n",
      "                        plateau is used, this value is used for 'factor'\n",
      "  --step_size STEP_SIZE\n",
      "                        step_size for lr schedulers, if reduce on plateau,\n",
      "                        this value is used for 'patience'\n",
      "\n",
      "pl.Trainer:\n",
      "  --logger [LOGGER]     Logger (or iterable collection of loggers) for\n",
      "                        experiment tracking. A ``True`` value uses the default\n",
      "                        ``TensorBoardLogger``. ``False`` will disable logging.\n",
      "                        If multiple loggers are provided and the `save_dir`\n",
      "                        property of that logger is not set, local files\n",
      "                        (checkpoints, profiler traces, etc.) are saved in\n",
      "                        ``default_root_dir`` rather than in the ``log_dir`` of\n",
      "                        any of the individual loggers.\n",
      "  --checkpoint_callback [CHECKPOINT_CALLBACK]\n",
      "                        If ``True``, enable checkpointing. It will configure a\n",
      "                        default ModelCheckpoint callback if there is no user-\n",
      "                        defined ModelCheckpoint in :paramref:`~pytorch_lightni\n",
      "                        ng.trainer.trainer.Trainer.callbacks`.\n",
      "  --default_root_dir DEFAULT_ROOT_DIR\n",
      "                        Default path for logs and weights when no\n",
      "                        logger/ckpt_callback passed. Default: ``os.getcwd()``.\n",
      "                        Can be remote file paths such as `s3://mybucket/path`\n",
      "                        or 'hdfs://path/'\n",
      "  --gradient_clip_val GRADIENT_CLIP_VAL\n",
      "                        0 means don't clip.\n",
      "  --gradient_clip_algorithm GRADIENT_CLIP_ALGORITHM\n",
      "                        'value' means clip_by_value, 'norm' means\n",
      "                        clip_by_norm. Default: 'norm'\n",
      "  --process_position PROCESS_POSITION\n",
      "                        orders the progress bar when running multiple models\n",
      "                        on same machine.\n",
      "  --num_nodes NUM_NODES\n",
      "                        number of GPU nodes for distributed training.\n",
      "  --num_processes NUM_PROCESSES\n",
      "                        number of processes for distributed training with\n",
      "                        distributed_backend=\"ddp_cpu\"\n",
      "  --devices DEVICES     Will be mapped to either `gpus`, `tpu_cores`,\n",
      "                        `num_processes` or `ipus`, based on the accelerator\n",
      "                        type.\n",
      "  --gpus GPUS           number of gpus to train on (int) or which GPUs to\n",
      "                        train on (list or str) applied per node\n",
      "  --auto_select_gpus [AUTO_SELECT_GPUS]\n",
      "                        If enabled and `gpus` is an integer, pick available\n",
      "                        gpus automatically. This is especially useful when\n",
      "                        GPUs are configured to be in \"exclusive mode\", such\n",
      "                        that only one process at a time can access them.\n",
      "  --tpu_cores TPU_CORES\n",
      "                        How many TPU cores to train on (1 or 8) / Single TPU\n",
      "                        to train on [1]\n",
      "  --ipus IPUS           How many IPUs to train on.\n",
      "  --log_gpu_memory LOG_GPU_MEMORY\n",
      "                        None, 'min_max', 'all'. Might slow performance\n",
      "  --progress_bar_refresh_rate PROGRESS_BAR_REFRESH_RATE\n",
      "                        How often to refresh progress bar (in steps). Value\n",
      "                        ``0`` disables progress bar. Ignored when a custom\n",
      "                        progress bar is passed to\n",
      "                        :paramref:`~Trainer.callbacks`. Default: None, means a\n",
      "                        suitable value will be chosen based on the environment\n",
      "                        (terminal, Google COLAB, etc.).\n",
      "  --overfit_batches OVERFIT_BATCHES\n",
      "                        Overfit a fraction of training data (float) or a set\n",
      "                        number of batches (int).\n",
      "  --track_grad_norm TRACK_GRAD_NORM\n",
      "                        -1 no tracking. Otherwise tracks that p-norm. May be\n",
      "                        set to 'inf' infinity-norm.\n",
      "  --check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH\n",
      "                        Check val every n train epochs.\n",
      "  --fast_dev_run [FAST_DEV_RUN]\n",
      "                        runs n if set to ``n`` (int) else 1 if set to ``True``\n",
      "                        batch(es) of train, val and test to find any bugs (ie:\n",
      "                        a sort of unit test).\n",
      "  --accumulate_grad_batches ACCUMULATE_GRAD_BATCHES\n",
      "                        Accumulates grads every k batches or as set up in the\n",
      "                        dict.\n",
      "  --max_epochs MAX_EPOCHS\n",
      "                        Stop training once this number of epochs is reached.\n",
      "                        Disabled by default (None). If both max_epochs and\n",
      "                        max_steps are not specified, defaults to\n",
      "                        ``max_epochs`` = 1000.\n",
      "  --min_epochs MIN_EPOCHS\n",
      "                        Force training for at least these many epochs.\n",
      "                        Disabled by default (None). If both min_epochs and\n",
      "                        min_steps are not specified, defaults to\n",
      "                        ``min_epochs`` = 1.\n",
      "  --max_steps MAX_STEPS\n",
      "                        Stop training after this number of steps. Disabled by\n",
      "                        default (None).\n",
      "  --min_steps MIN_STEPS\n",
      "                        Force training for at least these number of steps.\n",
      "                        Disabled by default (None).\n",
      "  --max_time MAX_TIME   Stop training after this amount of time has passed.\n",
      "                        Disabled by default (None). The time duration can be\n",
      "                        specified in the format DD:HH:MM:SS (days, hours,\n",
      "                        minutes seconds), as a :class:`datetime.timedelta`, or\n",
      "                        a dictionary with keys that will be passed to\n",
      "                        :class:`datetime.timedelta`.\n",
      "  --limit_train_batches LIMIT_TRAIN_BATCHES\n",
      "                        How much of training dataset to check (float =\n",
      "                        fraction, int = num_batches)\n",
      "  --limit_val_batches LIMIT_VAL_BATCHES\n",
      "                        How much of validation dataset to check (float =\n",
      "                        fraction, int = num_batches)\n",
      "  --limit_test_batches LIMIT_TEST_BATCHES\n",
      "                        How much of test dataset to check (float = fraction,\n",
      "                        int = num_batches)\n",
      "  --limit_predict_batches LIMIT_PREDICT_BATCHES\n",
      "                        How much of prediction dataset to check (float =\n",
      "                        fraction, int = num_batches)\n",
      "  --val_check_interval VAL_CHECK_INTERVAL\n",
      "                        How often to check the validation set. Use float to\n",
      "                        check within a training epoch, use int to check every\n",
      "                        n steps (batches).\n",
      "  --flush_logs_every_n_steps FLUSH_LOGS_EVERY_N_STEPS\n",
      "                        How often to flush logs to disk (defaults to every 100\n",
      "                        steps).\n",
      "  --log_every_n_steps LOG_EVERY_N_STEPS\n",
      "                        How often to log within steps (defaults to every 50\n",
      "                        steps).\n",
      "  --accelerator ACCELERATOR\n",
      "                        Previously known as distributed_backend (dp, ddp,\n",
      "                        ddp2, etc...). Can also take in an accelerator object\n",
      "                        for custom hardware.\n",
      "  --sync_batchnorm [SYNC_BATCHNORM]\n",
      "                        Synchronize batch norm layers between process\n",
      "                        groups/whole world.\n",
      "  --precision PRECISION\n",
      "                        Double precision (64), full precision (32) or half\n",
      "                        precision (16). Can be used on CPU, GPU or TPUs.\n",
      "  --weights_summary WEIGHTS_SUMMARY\n",
      "                        Prints a summary of the weights when training begins.\n",
      "  --weights_save_path WEIGHTS_SAVE_PATH\n",
      "                        Where to save weights if specified. Will override\n",
      "                        default_root_dir for checkpoints only. Use this if for\n",
      "                        whatever reason you need the checkpoints stored in a\n",
      "                        different place than the logs written in\n",
      "                        `default_root_dir`. Can be remote file paths such as\n",
      "                        `s3://mybucket/path` or 'hdfs://path/' Defaults to\n",
      "                        `default_root_dir`.\n",
      "  --num_sanity_val_steps NUM_SANITY_VAL_STEPS\n",
      "                        Sanity check runs n validation batches before starting\n",
      "                        the training routine. Set it to `-1` to run all\n",
      "                        batches in all validation dataloaders.\n",
      "  --truncated_bptt_steps TRUNCATED_BPTT_STEPS\n",
      "                        Deprecated in v1.3 to be removed in 1.5. Please use :p\n",
      "                        aramref:`~pytorch_lightning.core.lightning.LightningMo\n",
      "                        dule.truncated_bptt_steps` instead.\n",
      "  --resume_from_checkpoint RESUME_FROM_CHECKPOINT\n",
      "                        Path/URL of the checkpoint from which training is\n",
      "                        resumed. If there is no checkpoint file at the path,\n",
      "                        start from scratch. If resuming from mid-epoch\n",
      "                        checkpoint, training will start from the beginning of\n",
      "                        the next epoch.\n",
      "  --profiler PROFILER   To profile individual steps during training and assist\n",
      "                        in identifying bottlenecks.\n",
      "  --benchmark [BENCHMARK]\n",
      "                        If true enables cudnn.benchmark.\n",
      "  --deterministic [DETERMINISTIC]\n",
      "                        If true enables cudnn.deterministic.\n",
      "  --reload_dataloaders_every_n_epochs RELOAD_DATALOADERS_EVERY_N_EPOCHS\n",
      "                        Set to a non-negative integer to reload dataloaders\n",
      "                        every n epochs. Default: 0\n",
      "  --reload_dataloaders_every_epoch [RELOAD_DATALOADERS_EVERY_EPOCH]\n",
      "                        Set to True to reload dataloaders every epoch. ..\n",
      "                        deprecated:: v1.4 ``reload_dataloaders_every_epoch``\n",
      "                        has been deprecated in v1.4 and will be removed in\n",
      "                        v1.6. Please use\n",
      "                        ``reload_dataloaders_every_n_epochs``.\n",
      "  --auto_lr_find [AUTO_LR_FIND]\n",
      "                        If set to True, will make trainer.tune() run a\n",
      "                        learning rate finder, trying to optimize initial\n",
      "                        learning for faster convergence. trainer.tune() method\n",
      "                        will set the suggested learning rate in self.lr or\n",
      "                        self.learning_rate in the LightningModule. To use a\n",
      "                        different key set a string instead of True with the\n",
      "                        key name.\n",
      "  --replace_sampler_ddp [REPLACE_SAMPLER_DDP]\n",
      "                        Explicitly enables or disables sampler replacement. If\n",
      "                        not specified this will toggled automatically when DDP\n",
      "                        is used. By default it will add ``shuffle=True`` for\n",
      "                        train sampler and ``shuffle=False`` for val/test\n",
      "                        sampler. If you want to customize it, you can set\n",
      "                        ``replace_sampler_ddp=False`` and add your own\n",
      "                        distributed sampler.\n",
      "  --terminate_on_nan [TERMINATE_ON_NAN]\n",
      "                        If set to True, will terminate training (by raising a\n",
      "                        `ValueError`) at the end of each training batch, if\n",
      "                        any of the parameters or the loss are NaN or +/-inf.\n",
      "  --auto_scale_batch_size [AUTO_SCALE_BATCH_SIZE]\n",
      "                        If set to True, will `initially` run a batch size\n",
      "                        finder trying to find the largest batch size that fits\n",
      "                        into memory. The result will be stored in\n",
      "                        self.batch_size in the LightningModule. Additionally,\n",
      "                        can be set to either `power` that estimates the batch\n",
      "                        size through a power search or `binsearch` that\n",
      "                        estimates the batch size through a binary search.\n",
      "  --prepare_data_per_node [PREPARE_DATA_PER_NODE]\n",
      "                        If True, each LOCAL_RANK=0 will call prepare data.\n",
      "                        Otherwise only NODE_RANK=0, LOCAL_RANK=0 will prepare\n",
      "                        data\n",
      "  --plugins PLUGINS     Plugins allow modification of core behavior like ddp\n",
      "                        and amp, and enable custom lightning plugins.\n",
      "  --amp_backend AMP_BACKEND\n",
      "                        The mixed precision backend to use (\"native\" or\n",
      "                        \"apex\")\n",
      "  --amp_level AMP_LEVEL\n",
      "                        The optimization level to use (O1, O2, etc...).\n",
      "  --distributed_backend DISTRIBUTED_BACKEND\n",
      "                        deprecated. Please use 'accelerator'\n",
      "  --move_metrics_to_cpu [MOVE_METRICS_TO_CPU]\n",
      "                        Whether to force internal logged metrics to be moved\n",
      "                        to cpu. This can save some gpu memory, but can make\n",
      "                        training slower. Use with attention.\n",
      "  --multiple_trainloader_mode MULTIPLE_TRAINLOADER_MODE\n",
      "                        How to loop over the datasets when there are multiple\n",
      "                        train loaders. In 'max_size_cycle' mode, the trainer\n",
      "                        ends one epoch when the largest dataset is traversed,\n",
      "                        and smaller datasets reload when running out of their\n",
      "                        data. In 'min_size' mode, all the datasets reload when\n",
      "                        reaching the minimum length of datasets.\n",
      "  --stochastic_weight_avg [STOCHASTIC_WEIGHT_AVG]\n",
      "                        Whether to use `Stochastic Weight Averaging (SWA)\n",
      "                        <https://pytorch.org/blog/pytorch-1.6-now-includes-\n",
      "                        stochastic-weight-averaging/>_`\n"
     ]
    }
   ],
   "source": [
    "!python experiment/train.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40209cfd1e49aba1e20a3908f9a243f43b2ed73034fd3a81730d62124bbdcdae"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
